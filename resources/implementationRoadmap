
# üß± OVERALL BUILD STRATEGY

You are building 5 core engines:

1. Log Ingestion Engine
2. Normalization Engine
3. Correlation + UEBA Engine
4. Pattern Cache Engine
5. LLM Playbook Engine

Each phase builds one layer.

You have **2 machines**:

* Machine 1 ‚Üí Target (simulated bank server)
* Machine 2 ‚Üí SOC Engine (your system)

---

üîµ PHASE 1 ‚Äî Infrastructure (Auth + Audit Only)
üéØ Objective

Continuously move only:

/var/log/auth.log

/var/log/audit/audit.log

from Machine 1 ‚Üí Machine 2.

üîê Machine 1 (Target)
Install Required Components
sudo apt update
sudo apt install auditd openssh-server

Enable Focused Audit Rules (Only What You Need)

Create /etc/audit/rules.d/soc.rules:

-w /etc/passwd -p wa -k passwd_changes
-w /etc/sudoers -p wa -k sudo_changes
-w /etc/shadow -p wa -k shadow_changes
-a always,exit -F arch=b64 -S execve -k command_exec
-a always,exit -F arch=b64 -S setuid,setgid -k privilege_change


Reload:

sudo augenrules --load
sudo systemctl restart auditd


Verify:

sudo ausearch -k command_exec


You now capture:

All command executions

Privilege changes

Critical file modifications

That‚Äôs enough to detect:

Brute force

Privilege escalation

Persistence

Suspicious command usage

üì¶ Machine 2 (SOC)

Install:

sudo apt install python3 python3-pip redis-server
pip install fastapi uvicorn pyod scikit-learn


(Omit tsfresh for now ‚Äî keep features handcrafted.)

Install Ollama:

curl -fsSL https://ollama.com/install.sh | sh
ollama pull mistral

üîÅ Log Transfer (Safer Method)

Instead of raw scp every 30 seconds (inefficient), use incremental rsync:

rsync -avz user@machine1:/var/log/auth.log ./logs/
rsync -avz user@machine1:/var/log/audit/audit.log ./logs/


Cron every 30‚Äì60 seconds.

‚úÖ End Phase 1

You have:

auth.log flowing

audit.log flowing

Verified continuous ingestion

No AI. No parsing yet.

üü¢ PHASE 2 ‚Äî Normalization Engine (Auth + Audit Only)

We build two focused parsers.

/soc_engine/
    ingestion/
        auth_parser.py
        audit_parser.py

üîê auth.log ‚Üí What You Extract

From sshd entries:

Example:

Failed password for invalid user admin from 10.0.0.5 port 445 ssh2


Extract:

{
  "timestamp": "",
  "event_type": "failed_login",
  "user": "admin",
  "source_ip": "10.0.0.5",
  "host": "",
  "command": null
}


Also detect:

Successful SSH login

sudo usage

session opened/closed

üõ† audit.log ‚Üí What You Extract

From:

type=EXECVE msg=audit(...)


Extract:

{
  "timestamp": "",
  "event_type": "command_exec",
  "user": "",
  "command": "/usr/bin/nmap",
  "source_ip": null
}


Also detect:

setuid/setgid ‚Üí privilege_change

passwd_changes

shadow_changes

Unified Schema (Same as Before)
{
  "timestamp": "",
  "host": "",
  "user": "",
  "event_type": "",
  "source_ip": "",
  "command": "",
  "raw": ""
}

SQLite Table (Add event_category)
CREATE TABLE events (
    id INTEGER PRIMARY KEY,
    timestamp TEXT,
    host TEXT,
    user TEXT,
    event_type TEXT,
    source_ip TEXT,
    command TEXT,
    anomaly_score REAL DEFAULT 0,
    incident_id TEXT
);

‚úÖ End Phase 2

You now have:

Structured SSH login data

Structured command execution data

Privilege changes captured

Everything stored locally

This is already a basic SIEM.

üü° PHASE 3 ‚Äî UEBA (Auth + Audit Focused)

Since you only have auth + audit, your behavioral features become:

üë§ Per User Behavior Profile

From auth.log:

Login frequency per hour

Failed login ratio

New IP detection

First login time baseline

From audit.log:

Command frequency

First-time command usage

High-risk command usage (nmap, nc, curl, wget, bash -i)

Privilege change count

üîé Feature Vector Example
[
  login_rate,
  failed_login_ratio,
  unique_ip_count,
  command_rate,
  new_command_flag,
  privilege_change_flag
]


Use:

from pyod.models.iforest import IForest


Isolation Forest works well here.

Threshold Logic

Since auth + audit only:

IF anomaly_score > 0.75
‚Üí suspicious

‚úÖ End Phase 3

System detects:

Abnormal SSH behavior

Abnormal command behavior

Abnormal privilege usage

üü† PHASE 4 ‚Äî Correlation (Auth + Audit Based Incidents)

Now incidents are built only from:

failed_login

successful_login

command_exec

privilege_change

passwd_changes

shadow_changes

üîó Correlation Rules (Focused)
1Ô∏è‚É£ Brute Force ‚Üí Success
IF failed_login > 5
AND successful_login within 5 min
‚Üí brute_force_success

2Ô∏è‚É£ Privilege Escalation
IF successful_login
AND privilege_change
AND anomaly_score high
‚Üí privilege_escalation

3Ô∏è‚É£ Persistence via Cron

From audit.log detect:

/etc/cron*

IF command_exec modifying cron
‚Üí persistence_attempt

4Ô∏è‚É£ Suspicious Command After Login
successful_login
‚Üí command_exec in <2 minutes
‚Üí high risk command

Incident Object
{
  "incident_id": "...",
  "type": "privilege_escalation",
  "user": "...",
  "host": "...",
  "severity": "high",
  "events": [...]
}

‚úÖ End Phase 4

You now generate meaningful incidents using only:

SSH data

System call data

That is clean and realistic.

# üî¥ PHASE 5 ‚Äî Pattern Fingerprinting + Cache (Days 15‚Äì17)

## üéØ Goal

Avoid recomputing known incidents.

---

## Step 1 ‚Äî Fingerprint Generator

Convert pattern features into deterministic string.

Hash using SHA256.

---

## Step 2 ‚Äî Cache System

Use Redis or SQLite table:

```
CREATE TABLE playbook_cache (
  pattern_id TEXT PRIMARY KEY,
  playbook TEXT,
  validated BOOLEAN
);
```

---

## Step 3 ‚Äî Cache Lookup Logic

Workflow:

```
incident ‚Üí fingerprint
if fingerprint in cache:
    return cached_playbook
else:
    call LLM
```

---

## ‚úÖ Deliverable

Repeat incidents return instant responses.

---

# üü£ PHASE 6 ‚Äî LLM Playbook Engine (Days 18‚Äì21)

## üéØ Goal

Generate response steps.

---

## Step 1 ‚Äî Prompt Engineering

Create structured prompt template:

```
You are a Linux SOC analyst.
Incident details:
...
Return structured JSON:
{
  "classification": "",
  "severity": "",
  "containment": [],
  "forensics": [],
  "remediation": []
}
```

---

## Step 2 ‚Äî Ollama Integration

```python
import ollama
response = ollama.chat(model="mistral", messages=[...])
```

Parse JSON output.

---

## Step 3 ‚Äî Validator Layer

Check if:

* Contains containment steps
* Mentions isolation
* Mentions log preservation

Reject vague output.

---

## Step 4 ‚Äî Store Approved Playbook

Insert into cache.

---

## ‚úÖ Deliverable

System generates structured playbooks offline.

---

# ‚ö´ PHASE 7 ‚Äî API + Dashboard (Days 22‚Äì25)

## üéØ Goal

Expose system cleanly.

Use FastAPI:

Endpoints:

```
GET /incidents
GET /incident/{id}
GET /playbook/{incident_id}
```

Optional:
Simple HTML dashboard.

---

# üß™ PHASE 8 ‚Äî Testing & Evaluation (Days 26‚Äì30)

Now simulate attacks.

## On Machine 2 (Attacker):

### 1. SSH Brute Force

### 2. Privilege escalation

### 3. Cron persistence

### 4. Data exfil simulation

---

## Measure:

* Detection time
* Correct classification
* Severity accuracy
* Playbook coverage
* Cache speed improvement

---

# üìÅ FINAL PROJECT STRUCTURE

```
soc_engine/
‚îÇ
‚îú‚îÄ‚îÄ ingestion/
‚îú‚îÄ‚îÄ normalization/
‚îú‚îÄ‚îÄ ueba/
‚îú‚îÄ‚îÄ correlation/
‚îú‚îÄ‚îÄ fingerprint/
‚îú‚îÄ‚îÄ cache/
‚îú‚îÄ‚îÄ llm/
‚îú‚îÄ‚îÄ api/
‚îú‚îÄ‚îÄ evaluation/
‚îî‚îÄ‚îÄ logs/
```

---

# üèÅ FINAL RESULT

You now have:

‚úî Simulated EDR (auditd)
‚úî Simulated SIEM (log aggregation)
‚úî UEBA anomaly detection
‚úî Correlation engine
‚úî Pattern fingerprinting
‚úî Cached known responses
‚úî Offline LLM playbook generation
‚úî Measurable evaluation

